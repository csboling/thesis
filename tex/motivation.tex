\documentclass[../thesis]{subfiles}

\begin{document}

\chapter{Motivation}

Poor reproducibility of scientific publications has been recognized as
a growing problem in a number of research areas, particularly those in
which experiments are complex and sensitive to small variations in
methodology. Large-scale replication efforts have suggested
reproducibility rates in some fields as low as 10\%
\cite{483531a}. Researchers have begun to call for improved
documentation of the scientific process in fields as diverse as
experimental psychology \cite{aac4716}, pharmaceutical research
\cite{483531a}, astronomy \cite{1601.07858}, and areas of computer
science such as machine learning \cite{OpenScienceML}. Much of the
recent attention paid to this issue comes in response to unique
characteristics the scientific process has developed in the digital
age, including unprecedented publication volume, insufficient
documentation of complex experimental procedures, and the availability
of software packages for manipulating statistics, but some critics
suspect that many long-standing results are also inadequately
verifiable. It has been suggested that factors such as publishing
pressure, confirmation bias, and inadequate statistical power of
hypotheses promote the widespread publication and citation of
unverifiable claims in all areas of experimental science
\cite{10.1371/journal.pmed.0020124}. To make matters worse, the last
few years have seen many cases of peer review fraud
\cite{Ferguson2014}, outright data falsification \cite{Fanelli2009},
and ethically dubious activities such as $p$-hacking \cite{Head2015},
the practice of massaging data to cross the accepted threshold of
statistical significance.

In addition to these systemic problems with the publication process,
the day-to-day practice of modern science and engineering research
presents a steadily growing host of data organization challenges to
investigators. Often an experimental program involves many personnel,
each with a unique specialization and research focus, performing
interdependent experiments at multiple universities. Each such experiment is
the product of a huge host of influences, and data are often collected
in ad-hoc or incompatible formats, making it difficult to draw honest
comparisons between results or to isolate methodological
problems. Especially in the case of technology development and
exploratory research, it is desirable for researchers in these kinds
of large projects to use uniform data acquisition protocols,
unambiguously describe their experimental procedures, and collate
their work into self-contained, consistently formatted units for
distribution to collaborators.

One proposition for improving the reproducibility of future scientific
work is to better standardize documentation practices for laboratory
procedures \cite{10.1371/journal.pmed.0020124} and ultimately to
transition from traditional paper-based publication models to
electronic formats which capture the intricacies of modern scientific
work. Ideally, a unit of disseminated research would provide enough
detail for future researchers to replicate every step of the
experiment and analysis associated with a publication and for
reviewers to identify sources of errors, details warranting further
examination, and academic misconduct. Confirmation studies as well as
exploratory research could benefit from the adoption of flexible
software tools for collecting data and chronicling experimental
procedures. Particularly in ``\textit{\gls{insilico}}'' fields, where
experiments consist of the transformation and analysis of data sets
within the digital domain, research stands to benefit from
software that can automate and standardize tasks such as
experimental design and record keeping, and some publication
organizations have begun to encourage sharing of code, procedures, and
raw data alongside submitted manuscripts.
Software tools for managing complex simulation and data analysis
pipelines have begun to emerge in recent years which offer support for a number of powerful features,
including data provenance, sharing and refining workflows, and
packaging execution environments into virtual machines for
later execution on different hardware \cite{VisTrails, Taverna}.
These tools typically do not attempt to model or automate non-software
research tasks in detail.
To address some of the informatics challenges of more ``hands-on''
research, several companies have developed
so-called \gls{LIMS}, which are
better suited to the inventory and data management needs of
traditional scientific facilities such as wet labs.
However, many scientific endeavors involve some mixture of
structuring \textit{\gls{insilico}} analysis workflows and directly
manipulating physical systems, and software toolchains for uniformly
managing procedures of this nature remain immature.

In fields where research involves both sophisticated software analysis
and intensive batteries of physical experiments, investigators could
benefit from a software platform which unifies protocol design, data
acquisition, result annotation and archiving, signal processing, and
other tasks involved in the complete research and development life
cycle. Such a tool should be (i) automatic, employing computer control
whenever possible to produce organized, uniform and repeatable
experiments; (ii) extensible and modular, promoting adoption of new
equipment, experimental methods, and data analysis techniques via
user-crafted plugins; (iii) collaborative, allowing results and
proposed experiments to be shared, annotated, and reviewed at many
levels of detail; (iv) bespoke, accommodating and complementing the
focus of each researcher involved in an interdisciplinary research and
development project, and (v) provenance-aware, enabling fine-grained
differential analysis of experimental outcomes and
methodologies. Existing approaches do not combine data acquisition and
archiving features with interfaces for process customization in a way
that meets all the above goals. In many cases tools built for
these purposes are also insufficiently adaptable for the fast-paced
and varied needs of active scientists, causing users to abandon the
software once it presents more limitations than benefits.

Fortunately, modern web technologies have begun to enable software
design strategies that make a complex, customizable end-to-end
solution feasible. Network-enabled services with diverse purposes and
internal infrastructures have become increasingly interoperable thanks
to the adoption of self-documenting web \glspl{API}. The increasing
sophistication of web browsers has allowed for an explosion of rich
client-side software experiences, enabling full-featured and user
interfaces which are platform-independent and easily updated. A number
of technologies such as distributed version control, demand-scaling
cloud hosting services, and real-time full-duplex network data
streaming have emerged as powerful tools for rapidly building robust
and flexible web applications with unprecedented capabilities. The
availability of inexpensive sensor and network hardware has begun to
spur the growth of the emerging \gls{IoT}, a vision of the near future
in which ubiquitous computing devices collect data, communicate with
each other, and interact with their environments.  Together these
advancements provide a rich software ecosystem for implementing a
next-generation \gls{LIMS} for performing complex experiments,
curating detailed data sets, and generating publication units with
end-to-end reproducibility.

This thesis describes the design and implementation of a suite of
software tools for data acquisition and provenance tracking with the
goal of leveraging computer automation to create a scientific
dissemination format with rich facilities for comparing results,
identifying new directions of research, and fostering collaboration
than traditional print publications. The design of the described
software tool embraces modern web technologies, separating functional
units into independent networked services which communicate by
discoverable web \glspl{API}. This architecture enables investigators
to interact with each other's research remotely and to independently
create reusable services of their own. The core components of the
system are modular and loosely coupled, and users are encouraged to
modify, create, and share software components to meet the unique needs
of their research. By designing a modular architecture which
anticipates rapidly changing requirements and enables users to take an
active role in software maintenance, the platform is intended to grow
with its user base and enjoy broader usefulness and greater longevity
than existing free and commercial lab informatics packages.  The
framework provides high-level capabilities for remotely controlling
lab equipment and routing captured sensor data, with a vision of
connecting research labs to the nascent \gls{IoT}. To demonstrate and
explore the system's capabilities, a embedded system was developed for
performing customizable electrochemical experiments, which includes a
multi-channel arbitrary waveform generator. The system's architecture
is described in detail along with an overview of the developers'
implementation choices. Throughout the exposition, the design and
integration of multiple custom electrochemical instruments serve to
demonstrate how users might add and modify software components to meet
the needs of their own research.

\end{document}


%%% Local Variables: ***
%%% mode:latex ***
%%% TeX-master: "../thesis.tex"  ***
%%% End: ***