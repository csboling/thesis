\chapter{Motivation}

Poor reproducibility of scientific publications has been
recognized as a growing problem in a number of research areas,
particularly those in which experiments are complex and sensitive to
small variations in methodology. Large-scale replication efforts have
suggested reproducibility rates in some fields as low as 10\%
\cite{483531a}. Researchers have begun to call for improved
documentation of the scientific process in fields as diverse as
experimental psychology \cite{aac4716}, pharmaceutical research
\cite{483531a}, astronomy \cite{1601.07858}, and areas of computer science such
as machine learning \cite{}. Much of the recent attention paid to this
issue comes in response to unique characteristics the scientific
process has developed in the digital age, including unprecedented publication
volume, insufficient documentation of complex experimental procedures,
and the availability of software packages for manipulating statistics,
but some critics suspect that many long-standing results are also
inadequately verifiable. It has been suggested that factors such as
publishing pressure, confirmation bias, and inadequate statistical
power of hypotheses promote the widespread publication and citation of
unverifiable claims in all areas of experimental science
\cite{10.1371/journal.pmed.0020124}. To make matters worse, the last
few years have seen many cases of peer review fraud
\cite{Ferguson2014}, outright data falsification \cite{Fanelli2009},
and ethically dubious activities such as $p$-hacking \cite{Head2015},
the practice of massaging data to cross the accepted threshold of
statistical significance.

In addition to these systemic problems with the publication process,
the day-to-day practice of modern science and engineering research
presents a steadily growing host of data organization challenges to
investigators. Often an experimental program involves many personnel,
each with a unique specialization and research focus, performing
interdependent experiments at multiple universities. Each such experiment is
the product of a huge host of influences, and data are often collected
in ad-hoc or incompatible formats, making it difficult to draw honest
comparisons between results or to isolate methodological
problems. Especially in the case of technology development and
exploratory research, it is desirable for researchers in these kinds
of large projects to use uniform data acquisition protocols,
unambiguously describe their experimental procedures, and collate
their work into self-contained, consistently formatted units for
distribution to collaborators.

One proposition for improving the
reproducibility of future scientific work is to better standardize
documentation practices for laboratory procedures
\cite{10.1371/journal.pmed.0020124} and ultimately to transition from
traditional paper-based publication models to electronic formats which
capture the intricacies of modern scientific work. Ideally, a unit of disseminated
research would provide enough detail for future researchers to replicate
every step of the experiment and analysis associated with a
publication and for reviewers to identify sources of errors, details
warranting further examination, and
academic misconduct. Confirmation studies as well as exploratory
research could benefit from the adoption of flexible
software tools for collecting data and chronicling experimental
procedures. Particularly in ``\textit{in-silico}'' fields, where
experiments consist of the transformation and analysis of data sets
within the digital domain, research stands to benefit from
software that can automate and standardize tasks such as
experimental design and record keeping, and some publication
organizations have begun to encourage sharing of code, procedures, and
raw data alongside submitted manuscripts.
Software tools for managing complex simulation and data analysis
pipelines have begun to emerge in recent years which offer support for a number of powerful features,
including data provenance, sharing and refining workflows, and
packaging execution environments into virtual machines for
later execution on different hardware \cite{VisTrails, Taverna}.
To address some of the informatics challenges of more ``hands-on''
research, several companies have developed
so-called laboratory information management systems (LIMS), which are
better suited to the inventory and data management needs of
traditional scientific facilities such as wet labs.
However, many scientific endeavors involve some mixture of
structuring \textit{in-silico} analysis workflows and directly
manipulating physical systems, and software toolchains for uniformly
managing procedures of this nature remain immature.

In fields where research involves both sophisticated software analysis
and intensive batteries of physical experiments, investigators could
benefit from a software platform which unifies protocol design, data
acquisition, result annotation and archiving, signal processing, and
other tasks involved in the complete research and development life
cycle. Such a tool should be (i) automatic, employing computer control
whenever possible to produce organized, uniform and repeatable
experiments; (ii) extensible and modular, promoting adoption of new
equipment, experimental methods, and data analysis techniques via
user-crafted plugins; (iii) collaborative, allowing results and
proposed experiments to be shared, annotated, and reviewed at many
levels of detail; (iv) bespoke, accommodating and complementing the
focus of each researcher involved in an interdisciplinary research and
development project, and (v) provenance-aware, enabling fine-grained
differential analysis of experimental outcomes and
methodologies. Existing approaches do not combine data acquisition and
archiving features with interfaces for process customization in a way
that meets all the above goals. In many cases tools built for
these purposes are also insufficiently adaptable for the fast-paced
and varied needs of active scientists, causing users to abandon the
software once it presents more limitations than benefits.

Fortunately, the rich landscape of modern web technologies has begun
to enable software design strategies that make a complex, customizable
end-to-end solution feasible. Network-enabled services with
diverse purposes and internal infrastructures have become increasingly
interoperable thanks to the adoption of self-documenting web APIs. The
increasing sophistication of web browsers has allowed for an
explosion of rich client-side software experiences, enabling
full-featured user interfaces which are platform-independent and easily
updated. A number of technologies such as distributed version control,
demand-scaling cloud hosting services, and real-time full-duplex
network data streaming have emerged as powerful tools for
rapidly building robust and flexible web applications. The
availability of inexpensive sensor and network hardware has begun to
spur the growth of the emerging Internet of Things, a vision of the
near future in which ubiquitous computing devices collect data,
communicate with each other, and interact with their environments.

This thesis describes the design and implementation of a suite of
software tools for data acquisition and provenance
tracking with the goal of leveraging computer automation to create a
scientific dissemination format with rich facilities for
comparing results, identifying new directions of research, and fostering
collaboration than traditional print publications. The design of the
described software tool embraces modern web technologies, separating
functional units into independent networked services which communicate
by discoverable web APIs. This architecture enables investigators to
interact with each others' research remotely and to independently
create reusable services of their own. The core components of the system are
modular and loosely coupled, and users are encouraged to modify,
create, and share software components to meet the unique needs of
their research. By designing a modular architecture which anticipates rapidly
changing requirements and enables users to take an active role in
software maintenance, the platform is intended to grow with its user
base and enjoy broader usefulness and greater longevity than existing
free and commercial lab informatics packages.
The framework provides high-level capabilities for
remotely controlling lab equipment and routing captured sensor data,
with a vision of connecting research labs to the nascent internet of
things. To demonstrate and explore the system's capabilities, a
embedded system was developed for performing customizable
electrochemical experiments, which includes a multi-channel arbitrary
waveform generator.