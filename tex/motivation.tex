\chapter{Motivation}

Poor reproducibility of scientific publications has been
recognized as a growing problem in a number of research areas,
particularly experimental psychology \cite{}, pharmaceutical research
\cite{}, astronomy \cite{}, and areas of computer science such as
machine learning \cite{}. Meta-analyses have suggested reproducibility rates
in some fields as low as [FIGURE] \cite{}. Much of the recent
attention paid to this issue comes in response to unique
characteristics of the scientific process in the digital age,
including unprecedented publication volume, insufficient documentation
of complex experimental procedures, and so-called data dredging
\cite{}, but some critics suspect that many long-standing results
are also inadequately verifiable \cite{}. It has been suggested that
factors such as publishing pressure, confirmation bias, and inadequate
statistical power of hypotheses promote the widespread publication and
citation of unverifiable claims in all areas of experimental science
\cite{}. To make matters worse, the last few years have seen many
cases of peer review fraud \cite{}, outright
data falsification \cite{}, and ethically dubious activities such as
$p$-hacking \cite{}, the practice of massaging data to cross the
accepted threshold of statistical significance. Even researchers in
strictly theoretical fields have expressed concerns about the
trustworthiness of some published results, given that crucial flaws
have been found buried in lengthy and technical proofs in pure
mathematics \cite{} and certain areas of theoretical physics concern
regimes which may never be amenable to experimental validation
\cite{}. One proposition for improving the reproducibility of future
scientific work is increased standardization and
transparency of laboratory procedures \cite{}. Ideally, publication
units would provide enough detail for future researchers to replicate
every step of the experiment and data analysis and for reviewers to
identify the source of errors and academic misconduct.

In addition to these systemic problems with the publication process,
the day-to-day practice of modern science and engineering research
presents a host of data organization challenges to
investigators. Often an experimental program involves many personnel,
each with a unique specialization and research focus, performing
interdependent experiments at multiple universities. Each such experiment is
the product of a huge host of influences, and data are often collected
in ad-hoc or incompatible formats, making it difficult to draw honest
comparisons between results or to isolate methodological
problems. Especially in the case of technology development and
exploratory research, it is desirable for researchers in these kinds
of large projects to use uniform data acquisition protocols,
unambiguously describe their experimental procedures, and collate
their work into self-contained, consistently formatted units for
distribution to collaborators.


the adoption of flexible, transparent software tools for collecting
data and chronicling experimental procedures. By attaching provenance
information to each dataset,
computer-assisted data provenance tracking, leveraging computational
tools to promote a scientific dissemination format equipped with
richer faculties for comparison, , and collaboration than traditional
print publications.

Careful bookkeeping